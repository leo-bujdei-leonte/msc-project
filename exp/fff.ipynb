{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "import math\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"../data/image/MNIST/MNIST_SLIC_graph_28_16_0p5.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_stack_slic_graph_patches(data, size):\n",
    "    r = Resize(size)\n",
    "    \n",
    "    for g in data:\n",
    "        g.imgs = [torch.Tensor(img).unsqueeze(0) for img in g.imgs]\n",
    "        g.imgs = [r(img) for img in g.imgs]\n",
    "        g.imgs = torch.cat(g.imgs, dim=0).unsqueeze(1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "dataset = resize_stack_slic_graph_patches(dataset, (7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False,  True]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_slic_graph_patches(batch):\n",
    "    lengths = torch.tensor([len(g.imgs) for g in batch])\n",
    "    max_len = torch.max(lengths)\n",
    "    mask = torch.arange(max_len).expand(len(lengths), max_len) >= lengths.unsqueeze(1)\n",
    "    \n",
    "    imgs = pad_sequence([g.imgs for g in batch], batch_first=True)\n",
    "    coords = pad_sequence([g.centroid for g in batch], batch_first=True)\n",
    "    \n",
    "    y = torch.tensor([g.y for g in batch], dtype=torch.long)\n",
    "\n",
    "    return imgs, coords, mask, y\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [.9, .1])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_slic_graph_patches)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_slic_graph_patches)\n",
    "\n",
    "next(iter(train_loader))[2][:,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7354, -2.4129, -2.8052, -3.4367, -1.9890, -2.0890, -3.1326, -3.3764,\n",
       "         -1.4232, -1.7512],\n",
       "        [-3.1527, -2.7185, -2.8222, -2.8696, -1.9462, -2.6661, -3.0450, -3.2003,\n",
       "         -1.5199, -1.3638],\n",
       "        [-2.2215, -2.1765, -2.9552, -2.8865, -1.8127, -2.5994, -3.5094, -3.2663,\n",
       "         -1.3715, -2.1985],\n",
       "        [-2.7306, -2.4969, -3.0589, -3.0459, -1.8857, -2.2834, -2.9170, -3.2802,\n",
       "         -1.2623, -2.0432],\n",
       "        [-2.5614, -2.8845, -3.2692, -2.6177, -2.0206, -2.0739, -2.2596, -3.2860,\n",
       "         -1.5621, -1.9231],\n",
       "        [-2.5108, -2.4781, -2.9143, -3.1705, -2.0552, -2.3170, -3.2491, -3.8691,\n",
       "         -1.0914, -2.1490],\n",
       "        [-2.3300, -2.6724, -2.5239, -2.7192, -2.2130, -2.0612, -2.6448, -3.9853,\n",
       "         -1.6115, -1.8222],\n",
       "        [-2.6251, -2.9441, -2.7898, -2.7099, -2.1250, -2.3106, -2.7228, -3.5696,\n",
       "         -1.0733, -2.3798],\n",
       "        [-1.6949, -2.6911, -3.0347, -2.5203, -1.8715, -2.6245, -2.9079, -3.2275,\n",
       "         -1.7247, -2.1101],\n",
       "        [-2.9135, -2.5316, -2.5592, -2.8758, -1.9196, -2.3258, -2.8371, -3.1418,\n",
       "         -1.2747, -2.2372],\n",
       "        [-2.1409, -2.5231, -3.0481, -2.8419, -2.1354, -2.3554, -2.6445, -3.4220,\n",
       "         -1.4097, -1.9987],\n",
       "        [-2.6438, -2.8531, -2.7861, -2.7119, -1.7430, -2.4759, -2.9402, -3.9180,\n",
       "         -1.4383, -1.7486],\n",
       "        [-2.5618, -2.5119, -2.8139, -2.6889, -2.0016, -1.9308, -2.8822, -3.4422,\n",
       "         -1.4465, -2.2050],\n",
       "        [-2.5291, -2.4337, -2.5505, -2.5175, -1.9392, -2.4809, -2.9718, -3.6934,\n",
       "         -1.3348, -2.2344],\n",
       "        [-2.3190, -2.7870, -2.5557, -2.7900, -1.8744, -2.1166, -2.8222, -3.5386,\n",
       "         -1.7149, -1.8416],\n",
       "        [-1.9811, -2.4819, -3.2145, -2.3208, -1.9687, -2.1026, -3.2500, -3.0029,\n",
       "         -1.8076, -2.0723],\n",
       "        [-3.0195, -2.7267, -3.0207, -2.7067, -1.9383, -2.6227, -2.9891, -3.2627,\n",
       "         -1.3126, -1.6301],\n",
       "        [-2.6496, -2.5781, -3.4614, -2.8684, -1.5441, -2.4548, -2.9324, -3.1251,\n",
       "         -1.6813, -1.7009],\n",
       "        [-2.8108, -2.7370, -2.9937, -3.1126, -1.9173, -1.9939, -2.5400, -2.9026,\n",
       "         -1.3618, -2.2314],\n",
       "        [-2.4610, -3.0109, -2.7001, -2.6774, -2.2123, -2.2832, -2.9648, -3.6778,\n",
       "         -1.4208, -1.6110],\n",
       "        [-2.7444, -2.8745, -2.6645, -3.6184, -1.6802, -2.0738, -2.9922, -4.0803,\n",
       "         -1.4277, -1.8090],\n",
       "        [-2.5678, -2.9582, -2.9563, -3.0085, -2.0684, -2.5161, -3.5936, -3.3532,\n",
       "         -1.4837, -1.2962],\n",
       "        [-2.5682, -2.2841, -3.1334, -2.9555, -1.4088, -2.2687, -2.6244, -3.3724,\n",
       "         -1.7843, -2.2705],\n",
       "        [-2.5980, -2.3650, -3.0232, -2.9219, -2.2441, -2.5995, -3.0446, -3.3624,\n",
       "         -1.2938, -1.6486],\n",
       "        [-2.7795, -2.7493, -2.8713, -2.4788, -1.9165, -2.5949, -2.8286, -2.9456,\n",
       "         -1.1650, -2.4288],\n",
       "        [-2.5753, -2.8746, -2.7098, -3.1181, -1.7910, -2.7305, -2.6958, -3.1268,\n",
       "         -1.3992, -1.7927],\n",
       "        [-2.6237, -2.0754, -2.6453, -2.7026, -1.8875, -2.9095, -3.2514, -3.6946,\n",
       "         -1.5568, -1.6950],\n",
       "        [-2.3071, -2.5992, -2.6753, -2.9168, -1.8586, -2.0642, -3.3724, -3.3136,\n",
       "         -1.5665, -1.9600],\n",
       "        [-2.1655, -2.8327, -3.2935, -2.5960, -1.7692, -2.0205, -2.7575, -3.4331,\n",
       "         -1.4973, -2.3833],\n",
       "        [-2.7115, -2.6070, -2.3476, -3.2025, -1.8871, -2.5269, -2.8047, -3.6580,\n",
       "         -1.3295, -1.9574],\n",
       "        [-2.1230, -2.5920, -3.0317, -2.7715, -2.2884, -2.0822, -2.8010, -3.8322,\n",
       "         -1.3104, -2.1502],\n",
       "        [-2.5136, -2.1373, -2.7954, -2.3931, -1.8772, -2.6162, -3.1434, -2.9550,\n",
       "         -1.5909, -2.0911]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, dim, max_len=1000):\n",
    "        super(PositionalEncoding2D, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "        # self.register_buffer(\"pe_x\", pe.clone()) # TODO do we need this?\n",
    "        # self.register_buffer(\"pe_y\", pe.clone())\n",
    "    \n",
    "    def forward(self, coords): # TODO forces ints\n",
    "        pe_x = self.pe[coords[..., 0]]\n",
    "        pe_y = self.pe[coords[..., 1]]\n",
    "        \n",
    "        return pe_x + pe_y\n",
    "    \n",
    "    def to(self, device):\n",
    "        super(PositionalEncoding2D, self).to(device)\n",
    "        self.pe = self.pe.to(device)\n",
    "        return self\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    r\"\"\"Vision Encoder Model\n",
    "\n",
    "        An Encoder Layer with the added functionality to encode important local structures of a tokenized image\n",
    "\n",
    "        Args:\n",
    "            embed_size      (int): Embedding Size of Input\n",
    "            num_heads       (int): Number of heads in multi-headed attention\n",
    "            hidden_size     (int): Number of hidden layers\n",
    "            dropout         (float, optional): A probability from 0 to 1 which determines the dropout rate\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size: int, num_heads: int, hidden_size: int, dropout: float = 0.1):\n",
    "        super(VisionEncoder, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        # self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.embed_size)\n",
    "        self.norm2 = nn.LayerNorm(self.embed_size)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(self.embed_size, self.num_heads, dropout=dropout)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.embed_size, 4 * self.embed_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(4 * self.embed_size, self.embed_size),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        x = x.transpose(0, 1)\n",
    "        attn, _ = self.attention(x, x, x, key_padding_mask=mask)\n",
    "        x = x + attn\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class CoordViT(nn.Module):\n",
    "    r\"\"\"Vision Transformer Model\n",
    "\n",
    "        A transformer model to solve vision tasks by treating images as sequences of tokens.\n",
    "\n",
    "        Args:\n",
    "            image_size      (int): Size of input image\n",
    "            channel_size    (int): Size of the channel\n",
    "            patch_size      (int): Max patch size, determines number of split images/patches and token size\n",
    "            embed_size      (int): Embedding size of input\n",
    "            num_heads       (int): Number of heads in Multi-Headed Attention\n",
    "            classes         (int): Number of classes for classification of data\n",
    "            hidden_size     (int): Number of hidden layers\n",
    "            dropout         (float, optional): A probability from 0 to 1 which determines the dropout rate\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size: int, channel_size: int, patch_size: int, embed_size: int, num_heads: int,\n",
    "                 classes: int, num_layers: int, hidden_size: int, dropout: float = 0.1):\n",
    "        super(CoordViT, self).__init__()\n",
    "\n",
    "        self.p = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_size = channel_size * (patch_size ** 2)\n",
    "        self.num_heads = num_heads\n",
    "        self.classes = classes\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        self.embeddings = nn.Linear(self.patch_size, self.embed_size)\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, self.embed_size))\n",
    "        self.positional_encoding = PositionalEncoding2D(self.embed_size, self.image_size)\n",
    "\n",
    "        self.encoders = nn.ModuleList([])\n",
    "        for layer in range(self.num_layers):\n",
    "            self.encoders.append(VisionEncoder(self.embed_size, self.num_heads, self.hidden_size, self.dropout))\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.embed_size)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embed_size, self.classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, coords, mask):\n",
    "        b, n, c, h, w = x.size()\n",
    "\n",
    "        x = x.reshape(b, n, h*w)\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        b, n, e = x.size()\n",
    "\n",
    "        pe = self.positional_encoding(coords.int()).to(x.device) # TODO int\n",
    "        x = x + pe\n",
    "        \n",
    "        class_token = self.class_token.expand(b, 1, e)\n",
    "        x = torch.cat((x, class_token), dim=1)\n",
    "        mask = torch.cat((mask, torch.tensor([False] * mask.shape[0], device=mask.device).unsqueeze(1)), dim=1)\n",
    "        \n",
    "        x = self.dropout_layer(x)\n",
    "        \n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, mask)\n",
    "\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        x = F.log_softmax(self.classifier(self.norm(x)), dim=-1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def to(self, device):\n",
    "        super(CoordViT, self).to(device)\n",
    "        self.positional_encoding = self.positional_encoding.to(device)\n",
    "        return self\n",
    "\n",
    "\n",
    "image_size = 28\n",
    "channel_size = 1\n",
    "patch_size = 7\n",
    "embed_size = 512\n",
    "num_heads = 8\n",
    "classes = 10\n",
    "num_layers = 3\n",
    "hidden_size = 256\n",
    "dropout = 0.2\n",
    "model = CoordViT(image_size, channel_size, patch_size, embed_size, num_heads, classes, num_layers, hidden_size, dropout=dropout)\n",
    "\n",
    "x, coords, mask, y = next(iter(train_loader))\n",
    "model(x, coords, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: train loss 0.0271, train accuracy 0.708, test accuracy 0.904, time 20s\n",
      "Epoch 002: train loss 0.0105, train accuracy 0.894, test accuracy 0.931, time 19s\n",
      "Epoch 003: train loss 0.0081, train accuracy 0.918, test accuracy 0.943, time 20s\n",
      "Epoch 004: train loss 0.0067, train accuracy 0.931, test accuracy 0.949, time 21s\n",
      "Epoch 005: train loss 0.0060, train accuracy 0.939, test accuracy 0.955, time 21s\n",
      "Epoch 006: train loss 0.0053, train accuracy 0.945, test accuracy 0.955, time 22s\n",
      "Epoch 007: train loss 0.0048, train accuracy 0.951, test accuracy 0.959, time 22s\n",
      "Epoch 008: train loss 0.0044, train accuracy 0.955, test accuracy 0.960, time 23s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[188], line 201\u001b[0m\n\u001b[0;32m    197\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m    199\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m--> 201\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_epoch_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_epoch_coordvit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_coordvit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[188], line 150\u001b[0m, in \u001b[0;36mtrain_test_loop\u001b[1;34m(model, optimizer, criterion, train_loader, test_loader, num_epochs, lr_scheduler, save_path, plot, train_epoch_fn, eval_fn, normalise_loss)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_accs), num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    148\u001b[0m     interval \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 150\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalise_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalise_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m eval_fn(model, criterion, test_loader, normalise_loss\u001b[38;5;241m=\u001b[39mnormalise_loss)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[188], line 77\u001b[0m, in \u001b[0;36mtrain_epoch_coordvit\u001b[1;34m(model, optimizer, criterion, loader, normalise_loss)\u001b[0m\n\u001b[0;32m     75\u001b[0m out \u001b[38;5;241m=\u001b[39m model(x, coords, mask)\n\u001b[0;32m     76\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, y)\n\u001b[1;32m---> 77\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39meq(y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     79\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "import os\n",
    "from typing import Iterable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import graph, io, color\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def plot_train_test(x: Iterable, y_train: Iterable, y_test: Iterable,\n",
    "                    title: str, save_path: str, figsize: tuple[int, int] = (12, 8)) -> None:\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    plt.clf()\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(x, y_train)\n",
    "    plt.plot(x, y_test)\n",
    "    plt.legend([\"Train\", \"Test\"])\n",
    "    plt.title(title)\n",
    "    \n",
    "    if os.path.isfile(save_path):\n",
    "        os.remove(save_path)\n",
    "    plt.savefig(save_path)\n",
    "    \n",
    "    plt.clf()\n",
    "\n",
    "def train_epoch(model: nn.Module, optimizer: Optimizer, criterion: nn.Module,\n",
    "                loader: DataLoader, normalise_loss=True) -> tuple[float, float]:\n",
    "    model.train()\n",
    "    correct, total_loss, total = 0, 0, 0\n",
    "    for batch in loader:\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss.item()\n",
    "        correct += out.argmax(dim=-1).eq(y).sum().item()\n",
    "        total += len(y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if total == 0:\n",
    "        return 0, 1\n",
    "    \n",
    "    if normalise_loss:\n",
    "        total_loss /= total\n",
    "    \n",
    "    return total_loss, correct / total\n",
    "\n",
    "def train_epoch_coordvit(model: nn.Module, optimizer: Optimizer, criterion: nn.Module,\n",
    "                loader: DataLoader, normalise_loss=True) -> tuple[float, float]:\n",
    "    model.train()\n",
    "    correct, total_loss, total = 0, 0, 0\n",
    "    for batch in loader:\n",
    "        x, coords, mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(x, coords, mask)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss.item()\n",
    "        correct += out.argmax(dim=-1).eq(y).sum().item()\n",
    "        total += len(y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if total == 0:\n",
    "        return 0, 1\n",
    "    \n",
    "    if normalise_loss:\n",
    "        total_loss /= total\n",
    "    \n",
    "    return total_loss, correct / total\n",
    "\n",
    "def eval(model: nn.Module, criterion: nn.Module, loader: DataLoader, normalise_loss=True) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    correct, total_loss, total = 0, 0, 0\n",
    "    for batch in loader:\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        \n",
    "        out = model(x)\n",
    "        total_loss += criterion(out, y).item()\n",
    "        correct += out.argmax(dim=-1).eq(y).sum().item()\n",
    "        total += len(y)\n",
    "        \n",
    "    if total == 0:\n",
    "        return 0, 1\n",
    "    if normalise_loss:\n",
    "        total_loss /= total\n",
    "    return total_loss, correct / total\n",
    "\n",
    "def eval_coordvit(model: nn.Module, criterion: nn.Module, loader: DataLoader, normalise_loss=True) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    correct, total_loss, total = 0, 0, 0\n",
    "    for batch in loader:\n",
    "        x, coords, mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
    "        \n",
    "        out = model(x, coords, mask)\n",
    "        total_loss += criterion(out, y).item()\n",
    "        correct += out.argmax(dim=-1).eq(y).sum().item()\n",
    "        total += len(y)\n",
    "        \n",
    "    if total == 0:\n",
    "        return 0, 1\n",
    "    if normalise_loss:\n",
    "        total_loss /= total\n",
    "    return total_loss, correct / total\n",
    "\n",
    "def train_test_loop(model: nn.Module, optimizer: Optimizer, criterion: nn.Module,\n",
    "                    train_loader: DataLoader, test_loader: DataLoader,\n",
    "                    num_epochs: int, lr_scheduler: LRScheduler = None,\n",
    "                    save_path: str = None, plot=False,\n",
    "                    train_epoch_fn=train_epoch, eval_fn=eval,\n",
    "                    normalise_loss=True,\n",
    "                    ) -> tuple[list[float]]:\n",
    "    assert save_path is not None or plot == False\n",
    "    \n",
    "    train_accs, test_accs, train_losses, test_losses = [], [], [], []\n",
    "    \n",
    "    if save_path is not None:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(save_path + \"model.pt\"):\n",
    "            model.load_state_dict(torch.load(save_path + \"model.pt\"))\n",
    "        if os.path.exists(save_path + \"metrics.pkl\"):\n",
    "            metrics = pickle.load(open(save_path + \"metrics.pkl\", \"rb\"))\n",
    "            train_accs, test_accs, train_losses, test_losses = metrics\n",
    "            \n",
    "    \n",
    "    for i in range(1+len(train_accs), num_epochs+1):\n",
    "        interval = time()\n",
    "\n",
    "        train_loss, train_acc = train_epoch_fn(model, optimizer, criterion, train_loader, normalise_loss=normalise_loss)\n",
    "        test_loss, test_acc = eval_fn(model, criterion, test_loader, normalise_loss=normalise_loss)\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step(epoch=i, metrics=train_loss)\n",
    "\n",
    "        interval = time() - interval\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch {i:03d}: train loss {train_loss:.4f},\",\n",
    "            f\"train accuracy {train_acc:.3f},\",\n",
    "            f\"test accuracy {test_acc:.3f}, \"\n",
    "            f\"time {int(interval)}s\"\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        if save_path is not None:\n",
    "            torch.save(model.state_dict(), save_path + \"model.pt\")\n",
    "            pickle.dump(\n",
    "                (train_accs, test_accs, train_losses, test_losses),\n",
    "                open(save_path + \"metrics.pkl\", \"wb\"),\n",
    "            )\n",
    "        \n",
    "        if plot:\n",
    "            plot_train_test(list(range(1, i+1)), train_losses, test_losses,\n",
    "                            \"Loss\", save_path+\"loss.png\")\n",
    "            plot_train_test(list(range(1, i+1)), train_accs, test_accs,\n",
    "                            \"Classification accuracy\", save_path+\"accuracy.png\")\n",
    "\n",
    "    return train_losses, train_accs, test_losses, test_accs\n",
    "\n",
    "\n",
    "image_size = 28\n",
    "channel_size = 1\n",
    "patch_size = 7\n",
    "embed_size = 512\n",
    "num_heads = 8\n",
    "classes = 10\n",
    "num_layers = 3\n",
    "hidden_size = 256\n",
    "dropout = 0.2\n",
    "model = CoordViT(image_size, channel_size, patch_size, embed_size, num_heads, classes, num_layers, hidden_size, dropout=dropout).to(device)\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "metrics = train_test_loop(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    num_epochs,\n",
    "    save_path=None,\n",
    "    plot=False,\n",
    "    train_epoch_fn=train_epoch_coordvit,\n",
    "    eval_fn=eval_coordvit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing\n",
      "Epoch 001: train loss 0.0274, train accuracy 0.700, test accuracy 0.904, time 20s\n",
      "Epoch 002: train loss 0.0107, train accuracy 0.892, test accuracy 0.935, time 19s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 46\u001b[0m\n\u001b[0;32m     42\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m     44\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m---> 46\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_epoch_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_epoch_coordvit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_coordvit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[188], line 150\u001b[0m, in \u001b[0;36mtrain_test_loop\u001b[1;34m(model, optimizer, criterion, train_loader, test_loader, num_epochs, lr_scheduler, save_path, plot, train_epoch_fn, eval_fn, normalise_loss)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_accs), num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    148\u001b[0m     interval \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 150\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalise_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalise_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m eval_fn(model, criterion, test_loader, normalise_loss\u001b[38;5;241m=\u001b[39mnormalise_loss)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[188], line 71\u001b[0m, in \u001b[0;36mtrain_epoch_coordvit\u001b[1;34m(model, optimizer, criterion, loader, normalise_loss)\u001b[0m\n\u001b[0;32m     69\u001b[0m correct, total_loss, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m---> 71\u001b[0m     x, coords, mask, y \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     73\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     75\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(x, coords, mask)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = pickle.load(open(\"../data/image/MNIST/MNIST_SLIC_graph_28_16_0p5.pkl\", \"rb\"))\n",
    "dataset = resize_stack_slic_graph_patches(dataset, (7, 7)) # this is part of the model\n",
    "print(\"Finished preprocessing\")\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [.9, .1])\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_slic_graph_patches\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_slic_graph_patches\n",
    ")\n",
    "\n",
    "image_size = 28\n",
    "channel_size = 1\n",
    "patch_size = 7\n",
    "embed_size = 512\n",
    "num_heads = 8\n",
    "classes = 10\n",
    "num_layers = 3\n",
    "hidden_size = 256\n",
    "dropout = 0.2\n",
    "model = CoordViT(\n",
    "    image_size,\n",
    "    channel_size,\n",
    "    patch_size,\n",
    "    embed_size,\n",
    "    num_heads,\n",
    "    classes,\n",
    "    num_layers,\n",
    "    hidden_size,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "metrics = train_test_loop(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    num_epochs,\n",
    "    save_path=None,\n",
    "    plot=False,\n",
    "    train_epoch_fn=train_epoch_coordvit,\n",
    "    eval_fn=eval_coordvit,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
