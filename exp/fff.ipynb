{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage.segmentation import slic, mark_boundaries\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "import math\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# from torchvision.datasets import MNIST, CIFAR100\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.segmentation import slic\n",
    "from skimage.measure import regionprops\n",
    "from skimage import filters, graph\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.transforms import AddLaplacianEigenvectorPE\n",
    "\n",
    "from skimage.segmentation import slic\n",
    "from skimage.measure import regionprops\n",
    "from skimage import filters, graph\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Resize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def extract_patches(img, seg, reg):\n",
    "    imgs, masks, coords = [], [], []\n",
    "    for idx in range(len(reg)):\n",
    "        x_min, y_min, x_max, y_max = reg[idx].bbox\n",
    "        cropped_image = img[:, x_min:x_max, y_min:y_max]\n",
    "        cropped_mask = seg[x_min:x_max, y_min:y_max]\n",
    "        cropped_mask = cropped_mask == idx + 1\n",
    "        imgs.append(cropped_image)\n",
    "        masks.append(cropped_mask)\n",
    "        coords.append(reg[idx].centroid)\n",
    "\n",
    "    return imgs, masks, coords\n",
    "\n",
    "def random_split(data, ratios, dataset_name):\n",
    "    if len(ratios) == 2:\n",
    "        train_ratio, test_ratio = ratios\n",
    "        val_ratio = 0\n",
    "    elif len(ratios) == 3:\n",
    "        train_ratio, val_ratio, test_ratio = ratios\n",
    "    else:\n",
    "        raise ValueError(\"ratios must be of length 2 or 3\")\n",
    "    \n",
    "    save_path = os.sep.join([\".\", \"data\", \"split\", dataset_name, f\"{train_ratio}-{val_ratio}-{test_ratio}.pkl\"])\n",
    "    if os.path.isfile(save_path):\n",
    "        idx_train, idx_val, idx_test = pickle.load(open(save_path, \"rb\"))\n",
    "        print(\"Loaded existing data split\")\n",
    "        \n",
    "    else:\n",
    "        n = len(data)\n",
    "        t = [int(train_ratio*n), int((train_ratio+val_ratio)*n)]\n",
    "        \n",
    "        idx = torch.randperm(n)\n",
    "        idx_train, idx_val, idx_test = idx[:t[0]], idx[t[0]:t[1]], idx[t[1]:]\n",
    "        \n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        pickle.dump((idx_train, idx_val, idx_test), open(save_path, \"wb\"))\n",
    "        print(\"Created new data split\")\n",
    "    \n",
    "    train_data, val_data, test_data = [], [], []\n",
    "    for i in idx_train:\n",
    "        train_data.append(data[i])\n",
    "    for i in idx_val:\n",
    "        val_data.append(data[i])\n",
    "    for i in idx_test:\n",
    "        test_data.append(data[i])\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def resize_stack_slic_graph_patches(data, size):\n",
    "    r = Resize(size)\n",
    "\n",
    "    for idx, g in enumerate(data):\n",
    "        for i in range(len(g.imgs)):\n",
    "            g.imgs[i] = (torch.Tensor(g.imgs[i] * g.masks[i]) / torch.sum(torch.Tensor(g.masks[i]), dim=(0, 1))).unsqueeze(0)\n",
    "        # g.imgs = [torch.Tensor(g.imgs[i] * g.masks[i]).unsqueeze(0) for i in range(len(g.imgs))] \n",
    "        g.imgs = [r(img) for img in g.imgs]\n",
    "        g.imgs = torch.cat(g.imgs, dim=0)\n",
    "\n",
    "        if len(g.imgs.shape) == 3:\n",
    "            g.imgs = g.imgs.unsqueeze(1)\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(\"Processed graph\", idx)\n",
    "\n",
    "    return data\n",
    "\n",
    "def collate_slic_graph_patches(batch):\n",
    "    lengths = torch.tensor([len(g.imgs) for g in batch])\n",
    "    max_len = torch.max(lengths)\n",
    "    mask = torch.arange(max_len).expand(len(lengths), max_len) >= lengths.unsqueeze(1)\n",
    "\n",
    "    imgs = pad_sequence([g.imgs for g in batch], batch_first=True)\n",
    "    coords = pad_sequence([g.centroid for g in batch], batch_first=True)\n",
    "\n",
    "    y = torch.tensor([g.y for g in batch], dtype=torch.long)\n",
    "\n",
    "    return imgs, coords, mask, y\n",
    "\n",
    "def generate_all_edges(h, w):\n",
    "    directions = [\n",
    "        (-1, 0),  # left\n",
    "        (-1, -1), # top-left\n",
    "        (0, -1),  # top\n",
    "        (1, -1),  # top-right\n",
    "        (1, 0),   # right\n",
    "        (1, 1),   # bottom-right\n",
    "        (0, 1),   # bottom\n",
    "        (-1, 1)   # bottom-left\n",
    "    ]\n",
    "    \n",
    "    # Generate all pixel coordinates (x, y)\n",
    "    x_coords = torch.arange(w)\n",
    "    y_coords = torch.arange(h)\n",
    "    \n",
    "    # Create a grid of all pixel coordinates\n",
    "    grid_x, grid_y = torch.meshgrid(x_coords, y_coords, indexing='ij')\n",
    "    \n",
    "    # Flatten the grid to get all (x, y) pairs\n",
    "    grid_x = grid_x.flatten()\n",
    "    grid_y = grid_y.flatten()\n",
    "    \n",
    "    # Initialize lists to collect valid edges\n",
    "    source_indices = []\n",
    "    target_indices = []\n",
    "    \n",
    "    # Loop through all directions\n",
    "    for dx, dy in directions:\n",
    "        # Compute the target coordinates (x + dx, y + dy)\n",
    "        target_x = grid_x + dx\n",
    "        target_y = grid_y + dy\n",
    "        \n",
    "        # Determine valid edges (those within the image bounds)\n",
    "        valid_mask = (target_x >= 0) & (target_x < w) & (target_y >= 0) & (target_y < h)\n",
    "        \n",
    "        # Extract valid coordinates\n",
    "        valid_source_indices = grid_y[valid_mask] * w + grid_x[valid_mask]\n",
    "        valid_target_indices = target_y[valid_mask] * w + target_x[valid_mask]\n",
    "        \n",
    "        # Append to the lists\n",
    "        source_indices.append(valid_source_indices)\n",
    "        target_indices.append(valid_target_indices)\n",
    "    \n",
    "    # Concatenate all valid edges\n",
    "    all_source_indices = torch.cat(source_indices)\n",
    "    all_target_indices = torch.cat(target_indices)\n",
    "    \n",
    "    # Stack the source and target indices to form the edges\n",
    "    edges = torch.stack([all_source_indices, all_target_indices], dim=0)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def image_to_pygraph(data):\n",
    "    img, y = data\n",
    "    c, h, w = img.shape\n",
    "    \n",
    "    edge_index = generate_all_edges(h, w)\n",
    "    \n",
    "    x = img.reshape(c, -1).T\n",
    "    \n",
    "    coords = torch.cartesian_prod(torch.arange(h), torch.arange(w))\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, y=y, coords=coords)\n",
    "\n",
    "def image_to_SLIC_graph(data, n_segments=14*14, compactness=0.5, save_img=False):\n",
    "    img, y = data\n",
    "    \n",
    "    assert type(img) == torch.Tensor and len(img.shape) == 3 and (img.shape[0] == 1 or img.shape[0] == 3)\n",
    "\n",
    "    num_channels = img.shape[0]\n",
    "    img_np = np.array(img.permute(1, 2, 0)) if num_channels == 3 else np.array(img.squeeze(0))\n",
    "    seg = slic(img_np, n_segments=n_segments, compactness=compactness, channel_axis=-1 if num_channels == 3 else None)\n",
    "    reg = regionprops(seg)\n",
    "\n",
    "    edge_boundary = filters.sobel(img_np if num_channels == 1 else np.mean(img_np, axis=2))\n",
    "    nx_g = graph.rag_boundary(seg, edge_boundary)\n",
    "    g = from_networkx(nx_g)\n",
    "    if save_img:\n",
    "        g.img = img_np\n",
    "        g.seg = seg\n",
    "        g.edge_boundary = edge_boundary\n",
    "\n",
    "    imgs, masks, coords = extract_patches(img, seg, reg)\n",
    "    g.centroid = torch.Tensor([coords[label[0] - 1] for label in g.labels])\n",
    "    g.imgs = [imgs[label[0] - 1] for label in g.labels]\n",
    "    g.masks = [masks[label[0] - 1] for label in g.labels]\n",
    "    g.y = y\n",
    "\n",
    "    return g\n",
    "\n",
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(self, root, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.data = []\n",
    "        self.train_data, self.val_data, self.test_data = [], [], []\n",
    "        self.train_loader, self.val_loader, self.test_loader = None, None, None\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def to_pixel_graphs(self, laplacian_pe=False, **kwargs):\n",
    "        g_path = os.sep.join([self.root, \"graph\", \"pixel.pkl\"])\n",
    "        if os.path.isfile(g_path):\n",
    "            self.data = pickle.load(open(g_path, \"rb\"))\n",
    "            print(\"Loaded existing pixel graphs\")\n",
    "        \n",
    "        else:\n",
    "            data = []\n",
    "            for idx, datum in enumerate(self.data):\n",
    "                data.append(image_to_pygraph(datum))\n",
    "                if idx % 1000 == 0:\n",
    "                    print(\"Processed image\", idx)\n",
    "            os.makedirs(os.path.dirname(g_path), exist_ok=True)\n",
    "            pickle.dump(data, open(g_path, \"wb\"))\n",
    "            print(\"Converted image dataset to pixel graphs\")\n",
    "            \n",
    "            self.data = data\n",
    "        \n",
    "        if laplacian_pe: # not recommended; use in the model unless overhead is too high\n",
    "            transform = AddLaplacianEigenvectorPE(**kwargs)\n",
    "            for idx, datum in enumerate(self.data):\n",
    "                pe = transform(datum).laplacian_eigenvector_pe\n",
    "                datum.x = torch.cat((datum.x, pe), dim=-1)\n",
    "                if idx % 1000 == 0:\n",
    "                    print(\"Processed image\", idx)\n",
    "            print(\"Precomputed Laplacian PE\")\n",
    "            \n",
    "    \n",
    "    def _convert_to_slic(self, n_segments, compactness):\n",
    "        g_path = os.sep.join([self.root, \"graph\", f\"SLIC_{n_segments}_{compactness}.pkl\"])\n",
    "        if os.path.isfile(g_path):\n",
    "            data = pickle.load(open(g_path, \"rb\"))\n",
    "            print(\"Loaded existing SLIC graphs\")\n",
    "        \n",
    "        else:\n",
    "            data = []\n",
    "            for idx, datum in enumerate(self.data):\n",
    "                data.append(image_to_SLIC_graph(datum, n_segments, compactness))\n",
    "                if idx % 1000 == 0:\n",
    "                    print(\"Processed image\", idx)\n",
    "            os.makedirs(os.path.dirname(g_path), exist_ok=True)\n",
    "            pickle.dump(data, open(g_path, \"wb\"))    \n",
    "            print(\"Converted image dataset to SLIC graphs\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def to_slic_graphs(self, n_segments, compactness, resize_stack_patches=None):\n",
    "        if resize_stack_patches is not None:\n",
    "            g_path = os.sep.join([self.root, \"graph\", f\"SLIC_{n_segments}_{compactness}_resized_stacked_{resize_stack_patches}.pkl\"])\n",
    "            if os.path.isfile(g_path):\n",
    "                self.data = pickle.load(open(g_path, \"rb\"))\n",
    "                print(\"Loaded existing resized and stacked graphs\")\n",
    "            \n",
    "            else:\n",
    "                self.data = self._convert_to_slic(n_segments, compactness)\n",
    "                \n",
    "                print(\"Resizing and stacking patches\")\n",
    "                self.data = resize_stack_slic_graph_patches(self.data, resize_stack_patches)\n",
    "                os.makedirs(os.path.dirname(g_path), exist_ok=True)\n",
    "                pickle.dump(self.data, open(g_path, \"wb\"))\n",
    "        \n",
    "        else:\n",
    "            self.data = self._convert_to_slic(n_segments, compactness)\n",
    "    \n",
    "    def splits(self, ratios):\n",
    "        assert sum(ratios) == 1\n",
    "\n",
    "        if self.train_data == []:\n",
    "            self.train_data, self.val_data, self.test_data = random_split(self.data, ratios, \"CIFAR100\")\n",
    "        return self.train_data, self.val_data, self.test_data\n",
    "\n",
    "    def loaders(self, batch_size, shuffles=(True, False, False), num_workers=0, graph_loader=False, collate_fn=None):\n",
    "        LoaderClass = PyGDataLoader if graph_loader else DataLoader\n",
    "        if self.train_loader is None:\n",
    "            self.train_loader = LoaderClass(\n",
    "                self.train_data,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffles[0],\n",
    "                num_workers=num_workers,\n",
    "                collate_fn=collate_fn,\n",
    "            )\n",
    "            self.val_loader = LoaderClass(\n",
    "                self.val_data,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffles[1],\n",
    "                num_workers=num_workers,\n",
    "                collate_fn=collate_fn,\n",
    "            )\n",
    "            self.test_loader = LoaderClass(\n",
    "                self.test_data,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffles[2],\n",
    "                num_workers=num_workers,\n",
    "                collate_fn=collate_fn,\n",
    "            )\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n",
    "    \n",
    "class CIFAR100(ImageClassificationDataset):\n",
    "    def __init__(self, root, transform=None, download=True) -> None:\n",
    "        super().__init__(root=root)\n",
    "        \n",
    "        self.data = datasets.CIFAR100(root, transform=transform, download=download)\n",
    "\n",
    "\n",
    "class MNIST(ImageClassificationDataset):\n",
    "    def __init__(self, root, transform=None, download=True) -> None:\n",
    "        super().__init__(root=root)\n",
    "        \n",
    "        self.data = datasets.MNIST(root, transform=transform, download=download)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = Compose([\n",
    "    Resize((32, 32)),\n",
    "    ToTensor(),\n",
    "    Normalize(0, 1),\n",
    "])\n",
    "dataset = CIFAR100(root=\"../data/image/CIFAR100\", download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing SLIC graphs\n"
     ]
    }
   ],
   "source": [
    "resize_stack_patches = (7, 7)\n",
    "n_segments = 16\n",
    "compactness = 15\n",
    "dataset.to_slic_graphs(resize_stack_patches=None, n_segments=n_segments, compactness=compactness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ True, False, False],\n",
       "        [ True, False,  True],\n",
       "        [False, False,  True]]),\n",
       " array([[[6, 7, 2],\n",
       "         [2, 5, 7],\n",
       "         [3, 6, 7]],\n",
       " \n",
       "        [[2, 0, 3],\n",
       "         [7, 8, 5],\n",
       "         [8, 9, 2]]]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.random.randint(10, size=(2, 3, 3))\n",
    "mask = np.random.randint(2, size=(3, 3))\n",
    "mask.astype(bool), img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 0, 0],\n",
       "        [1, 0, 1],\n",
       "        [0, 0, 1]],\n",
       "\n",
       "       [[1, 0, 0],\n",
       "        [1, 0, 1],\n",
       "        [0, 0, 1]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.reshape(1, mask.shape[0], mask.shape[1]).repeat(img.shape[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_array(data=[7, 7],\n",
       "             mask=[False, False],\n",
       "       fill_value=999999)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_img = np.ma.masked_array(img, ~mask.reshape(1, mask.shape[0], mask.shape[1]).repeat(img.shape[0], axis=0).astype(bool))\n",
    "masked_img.mean(axis=(-1,-2))\n",
    "masked_img.std(axis=(-1,-2))\n",
    "masked_img.min(axis=(-1,-2))\n",
    "masked_img.max(axis=(-1,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8540, 0.2255, 0.0863, 1.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lrgb_statistics(img, mask):\n",
    "    if len(img.shape) == 3:\n",
    "        expanded_mask = mask.reshape(1, mask.shape[0], mask.shape[1]).repeat(img.shape[0], axis=0)\n",
    "        masked_img = np.ma.masked_array(img, ~expanded_mask)\n",
    "        axes = (-1, -2)\n",
    "        res = torch.tensor(np.concatenate([\n",
    "            masked_img.mean(axis=axes),\n",
    "            masked_img.std(axis=axes),\n",
    "            masked_img.min(axis=axes),\n",
    "            masked_img.max(axis=axes),\n",
    "        ]))\n",
    "        return res\n",
    "    \n",
    "    elif len(img.shape) == 2:\n",
    "        masked_img = np.ma.masked_array(img, ~mask)\n",
    "        res = torch.tensor(np.array([\n",
    "            masked_img.mean(),\n",
    "            masked_img.std(),\n",
    "            masked_img.min(),\n",
    "            masked_img.max(),\n",
    "        ]))\n",
    "        return res\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "lrgb_statistics(b[0].imgs[0][0,:,:], b[0].masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed graph 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Data(edge_index=[2, 46], labels=[12, 1], weight=[46], count=[46], num_nodes=12, centroid=[12, 2], imgs=[12], masks=[12], y=19, batch=[1], x=[12, 12]),\n",
       " Data(edge_index=[2, 42], labels=[11, 1], weight=[42], count=[42], num_nodes=11, centroid=[11, 2], imgs=[11], masks=[11], y=29, x=[11, 12]),\n",
       " Data(edge_index=[2, 52], labels=[12, 1], weight=[52], count=[52], num_nodes=12, centroid=[12, 2], imgs=[12], masks=[12], y=0, x=[12, 12]),\n",
       " Data(edge_index=[2, 36], labels=[10, 1], weight=[36], count=[36], num_nodes=10, centroid=[10, 2], imgs=[10], masks=[10], y=11, x=[10, 12]),\n",
       " Data(edge_index=[2, 54], labels=[13, 1], weight=[54], count=[54], num_nodes=13, centroid=[13, 2], imgs=[13], masks=[13], y=1, x=[13, 12]),\n",
       " Data(edge_index=[2, 32], labels=[9, 1], weight=[32], count=[32], num_nodes=9, centroid=[9, 2], imgs=[9], masks=[9], y=86, x=[9, 12]),\n",
       " Data(edge_index=[2, 42], labels=[11, 1], weight=[42], count=[42], num_nodes=11, centroid=[11, 2], imgs=[11], masks=[11], y=90, x=[11, 12]),\n",
       " Data(edge_index=[2, 62], labels=[15, 1], weight=[62], count=[62], num_nodes=15, centroid=[15, 2], imgs=[15], masks=[15], y=28, x=[15, 12]),\n",
       " Data(edge_index=[2, 60], labels=[15, 1], weight=[60], count=[60], num_nodes=15, centroid=[15, 2], imgs=[15], masks=[15], y=23, x=[15, 12]),\n",
       " Data(edge_index=[2, 38], labels=[11, 1], weight=[38], count=[38], num_nodes=11, centroid=[11, 2], imgs=[11], masks=[11], y=31, x=[11, 12])]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lrgb_statistics(img, mask):\n",
    "    if len(img.shape) == 3:\n",
    "        expanded_mask = mask.reshape(1, mask.shape[0], mask.shape[1]).repeat(img.shape[0], axis=0)\n",
    "        masked_img = np.ma.masked_array(img, ~expanded_mask)\n",
    "        axes = (-1, -2)\n",
    "        res = torch.tensor(np.concatenate([\n",
    "            masked_img.mean(axis=axes),\n",
    "            masked_img.std(axis=axes),\n",
    "            masked_img.min(axis=axes),\n",
    "            masked_img.max(axis=axes),\n",
    "        ]))\n",
    "        return res\n",
    "    \n",
    "    elif len(img.shape) == 2:\n",
    "        masked_img = np.ma.masked_array(img, ~mask)\n",
    "        res = torch.tensor(np.array([\n",
    "            masked_img.mean(),\n",
    "            masked_img.std(),\n",
    "            masked_img.min(),\n",
    "            masked_img.max(),\n",
    "        ]))\n",
    "        return res\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "def slic_graph_patches_to_lrgb_stats(data):\n",
    "    # each patch becomes a 4c-dimensional embedding with mean, std, min, max\n",
    "    for idx, g in enumerate(data):\n",
    "        g.x = []\n",
    "        for i in range(len(g.imgs)):\n",
    "            g.x.append(lrgb_statistics(g.imgs[i], g.masks[i]))\n",
    "        g.x = torch.stack(g.x, dim=0)\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(\"Processed graph\", idx)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "slic_graph_patches_to_lrgb_stats(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
